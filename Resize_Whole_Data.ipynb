{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pprint\n",
    "import cv2\n",
    "from scipy.misc import imsave\n",
    "from helper import *\n",
    "from create_labels import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying Weighting algorithm to an image\n",
    "value = 1\n",
    "def weighting_algo(id_image):\n",
    "    '''\n",
    "    Creating weighting of an image\n",
    "    '''\n",
    "    image = id_image\n",
    "    x,y = image.shape\n",
    "    weighted_image = np.zeros((x,y))\n",
    "    for j in range(x):\n",
    "        for k in range(y):\n",
    "            # NOTE: did weights based on bold from legend\n",
    "            if(image[j][k]==1): # 1 red - myocardium\n",
    "                 w2 = 9 * value \n",
    "            elif(image[j][k] == 2): # blue - endocardium\n",
    "                 w2 = 10 * value\n",
    "            elif(image[j][k]== 3): # purple - fibrosis\n",
    "                 w2 = 4 * value \n",
    "            elif(image[j][k] == 4): # green - fat\n",
    "                 w2 = 11 * value\n",
    "            elif(image[j][k]== 5): # orange - dense collagen\n",
    "                 w2 = 5 * value\n",
    "            elif(image[j][k]== 6): # yellow - loose collagen\n",
    "                 w2 = 5 * value\n",
    "            elif(image[j][k]== 7):\n",
    "                 w2 = 6 * value # magenta - smooth muscle\n",
    "            else:\n",
    "                 w2 = 0\n",
    "            if(j!=0 and j!=x-1):\n",
    "                next_pix = image[j+1][k]\n",
    "                prev_pix = image[j-1][k]\n",
    "                # Taking the derivative of the pixels\n",
    "                if(np.absolute((next_pix-prev_pix))>0 and w2!=0):\n",
    "                    w1 = 10  \n",
    "                else:\n",
    "                    w1 = 0\n",
    "            else:\n",
    "                w1 = 0\n",
    "            w = 1 + w1 + w2\n",
    "            weighted_image[j][k] = w\n",
    "    return weighted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 black\n",
    "# 1 red - myocardium\n",
    "# 2 blue - endocardium\n",
    "# 3 purple - fibrosis\n",
    "# 4 green - fat\n",
    "# 5 orange - dense collagen\n",
    "# 6 yellow - loose collagen\n",
    "# 7 magenta - smooth muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting the directories\n",
    "import os\n",
    "\n",
    "# wanted_folder = 'alldata/'\n",
    "# wanted_folder = 'pruned/'\n",
    "# wanted_folder = 'Atrium/'\n",
    "wanted_folder = 'Ventricle/'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "check_directory = cwd\n",
    "if check_directory == '/home/sim/notebooks/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets/OCTData/'+wanted_folder\n",
    "elif check_directory == '/Users/sim/Desktop/Imperial/Project/PreTrained/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets-24-aug/OCTData/'+wanted_folder\n",
    "\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Raw Image\n",
    "whole_raw_image_folder = cwd + 'whole_raw_image/'\n",
    "brushlet_enhanced_folder = cwd + 'brushlet_enhanced/'\n",
    "brushlet_denoised_folder = cwd + 'brushlet_denoised/'\n",
    "\n",
    "# Labels\n",
    "manual_label_folder = cwd + 'manual_label/'\n",
    "all_labels_folder = cwd + 'all_labels/'\n",
    "sim_labels_folder = cwd + 'my_labels/'\n",
    "corrected_labels_folder = cwd + 'png_labels_method/'\n",
    "\n",
    "# Normalised Images\n",
    "normalised_folder = cwd + 'normalised_raw_image/'\n",
    "\n",
    "list_of_folders = [ whole_raw_image_folder, \\\n",
    "                    brushlet_denoised_folder, \\\n",
    "                    brushlet_enhanced_folder, \\\n",
    "                    manual_label_folder, \\\n",
    "                    all_labels_folder, \\\n",
    "                    sim_labels_folder, \\\n",
    "                    corrected_labels_folder, \\\n",
    "                    normalised_folder\n",
    "                  ]\n",
    "\n",
    "# Denoised Images\n",
    "# brushlet_enhanced_folder = cwd + 'normalised_brushlet_enhanced/'\n",
    "# brushlet_denoised_folder = cwd + 'normalised_brushlet_denoised/'\n",
    "# bm3d_folder = cwd + 'normalised_bm3d/'\n",
    "# nlm_folder = cwd + 'normalised_nl_means/'\n",
    "# bilateral_folder = cwd + 'normalised_bilateral_filter/'\n",
    "\n",
    "# # Denoised Images\n",
    "# brushlet_enhanced_folder = cwd + 'normalised_brushlet_enhanced/'\n",
    "# brushlet_denoised_folder = cwd + 'normalised_brushlet_denoised/'\n",
    "# bm3d_folder = cwd + 'normalised_bm3d/'\n",
    "# nlm_folder = cwd + 'normalised_nl_means/'\n",
    "# bilateral_folder = cwd + 'normalised_bilateral_filter/'\n",
    "\n",
    "# list_of_folders = [ whole_raw_image_folder, \\\n",
    "#                     brushlet_denoised_folder, \\\n",
    "#                     brushlet_enhanced_folder, \\\n",
    "#                     manual_label_folder, \\\n",
    "#                     all_labels_folder, \\\n",
    "#                     sim_labels_folder, \\\n",
    "#                     corrected_labels_folder, \\\n",
    "#                     normalised_folder,  \\\n",
    "#                     bm3d_folder,\\\n",
    "#                     nlm_folder,\\\n",
    "#                     bilateral_folder\n",
    "#                   ]\n",
    "print(len(list_of_folders))\n",
    "def list_all_files(directory):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    all_labels_files = [f for f in listdir(directory) if isfile(join(directory, f)) and f != '.DS_Store']\n",
    "    all_labels_files.sort()\n",
    "    return all_labels_files\n",
    "\n",
    "files_list = []\n",
    "for folders in list_of_folders:\n",
    "    files_list.append(list_all_files(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(files_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating list of file names\n",
    "new_file_list = []\n",
    "\n",
    "for i in files_list[0]:\n",
    "    if i[:4] == 'con_':\n",
    "        string = i[4:]\n",
    "        file_name = string[:19].replace('.','')\n",
    "    new_file_list.append(file_name)\n",
    "\n",
    "new_file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(new_file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing Unwanted Files\n",
    "unwanted_files = []\n",
    "test_set = [\"H1_N01848_LA_1_272\",\"H2_N02047_RVS_1_400\",\"H3_N02186_LA_1_400\",\"H3_N02186_LVS_1_676\",\"H4_N03210_RA_1_400\",\"H5_N03290_LV_1_420\",\"H6_N03320_LVS_1_400\",\"H6_N03320_LVS_1_455\",\"H7_N03555_LVS_1_306\",\"H7_N03555_RA_1_302\",\"H8_N03585_RVS_1_398\",\"H8_N03585_RA_1_345\",\"H9_N03857_LV_1_580\"]\n",
    "print(len(test_set))\n",
    "not_in = []\n",
    "for count, i in enumerate(new_file_list):\n",
    "    if i == \"H1_N01848_LA_1_272\":\n",
    "        print(i)\n",
    "    if i in test_set:\n",
    "        print(count,i)\n",
    "        not_in.append(i)\n",
    "        unwanted_files.append(count)\n",
    "print(unwanted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(unwanted_files))\n",
    "print(len(new_file_list))\n",
    "print(len(new_file_list)-len(unwanted_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Booosting underseperatted \n",
    "underrepresented = [\"H1_N01848_LA_1_272\",\"H1_N01848_LA_1_388\",\"H2_N02047_LA_1_241\",\"H2_N02047_LA_1_257\",\"H2_N02047_RA_1_380\",\"H3_N02186_LVS_1_676\",\"H3_N02186_RA_1_380\",\"H3_N02186_RA_1_40\",\"H4_N03210_LVS_1_40\",\"H5_N03290_LA_1_380\",\"H5_N03290_LA_1_400\",\"H6_N03320_LA_1_450\"]\n",
    "print(len(underrepresented))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Image Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def segment_image_multiple(image, left_bound, right_bound, split):\n",
    "    '''\n",
    "    Segment image at intervals of half of split to have more training data\n",
    "    \n",
    "    Segment image by input of image, left bound and right bound, and split int \n",
    "    '''\n",
    "    list_of_images = []\n",
    "    bounds = right_bound - left_bound\n",
    "    quot, rem = divmod(bounds, split)\n",
    "    ranged = np.arange(0,quot,0.25) # quadruple amount of data\n",
    "    ranged = ranged[:-2] \n",
    "    for i in ranged:\n",
    "        if len(image.shape) == 2:\n",
    "            cropped_image = crop_image(image, int(i*split), int((i+1) * split))\n",
    "            list_of_images.append(cropped_image)\n",
    "        elif len(image.shape) == 3:\n",
    "            cropped_image = crop_image(image, int(i*split), int((i+1) * split), False)\n",
    "            list_of_images.append(cropped_image)\n",
    "    return list_of_images\n",
    "\n",
    "whole_raw_1 = plt.imread(whole_raw_image_folder+files_list[0][0])\n",
    "segmented_whole = segment_image_multiple(whole_raw_1, 0, 600, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving images\n",
    "def crop_and_save_raw(image, src_direct, name):\n",
    "    dst_drc = src_direct + 'Train/'\n",
    "\n",
    "    # Crop all images\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600) # Crop all images at 512,600\n",
    "    elif len(image.shape) == 3:\n",
    "        height, width, colour = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600, colour) # Crop all images at 512,600\n",
    "    segmented_images = segment_image_multiple(image, 0, 600, 64)\n",
    "    \n",
    "#     print(len(segmented_images))\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=len(segmented_images), figsize=(9,20))\n",
    "#     for i, ax in enumerate(axes):\n",
    "#         ax.imshow(segmented_images[i])\n",
    "#         ax.set_title(i)\n",
    "#     plt.show()\n",
    "    # If in underrepresented, double image amount by horizontally flipping and adding to dataset\n",
    "    if name in underrepresented:\n",
    "        for i in range(len(segmented_images)):\n",
    "            image_name = dst_drc+name+'_'+str(i+1)+'.png'\n",
    "            cur_image = segmented_images[i]\n",
    "            imsave(image_name, cur_image)\n",
    "            \n",
    "            repeat_name = dst_drc+name+'_'+str(i+1+len(segmented_images))+'.png'\n",
    "            flipped_image = np.fliplr(segmented_images[i])\n",
    "            imsave(repeat_name, flipped_image)\n",
    "    else:\n",
    "        for i in range(len(segmented_images)):\n",
    "            image_name = dst_drc+name+'_'+str(i+1)+'.png'\n",
    "            cur_image = segmented_images[i]\n",
    "            imsave(image_name, cur_image)\n",
    "\n",
    "# Saving labels and weights\n",
    "def crop_and_save_label(label, src_direct, name):\n",
    "    id_dst_drc = src_direct + 'Train/segmented_ids/'\n",
    "    weight_dst_drc = src_direct + 'Train/weights/'\n",
    "    height, width = label.shape\n",
    "    \n",
    "    label[440:,:] = 0\n",
    "    \n",
    "    if width > 600:\n",
    "        label = crop_image(label, 0, 600)\n",
    "    if height != 512:\n",
    "        amount = 512 - height\n",
    "        id_full = np.zeros((amount,600))\n",
    "        label = np.concatenate((label, id_full))\n",
    "    \n",
    "    rgb_image = convert_to_rgb_image(label) \n",
    "    segmented_labels = segment_image_multiple(label, 0, 600, 64)\n",
    "    \n",
    "    # If in underrepresented, double image amount by horizontally flipping\n",
    "    if name in underrepresented:\n",
    "        for i in range(len(segmented_labels)):\n",
    "            id_name = id_dst_drc+'id_'+name+'_'+str(i+1)+'.npy'\n",
    "            weight_name = weight_dst_drc+'weight_'+name+'_'+str(i+1)+'.npy'\n",
    "            \n",
    "            # convert image_id into weighted image \n",
    "            cur_label = segmented_labels[i]\n",
    "            weighted_image = weighting_algo(cur_label)\n",
    "            \n",
    "            # Saving labels and weights\n",
    "            np.save(id_name,np.array(cur_label))\n",
    "            np.save(weight_name,np.array(weighted_image))\n",
    "            \n",
    "            repeat_id_name = id_dst_drc+'id_'+name+'_'+str(i+1+len(segmented_labels))+'.npy'\n",
    "            repeat_weight_name = weight_dst_drc+'weight_'+name+'_'+str(i+1+len(segmented_labels))+'.npy'\n",
    "            \n",
    "            # convert image_id into weighted image \n",
    "            flipped_label = np.fliplr(segmented_labels[i])\n",
    "            flipped_weight = np.fliplr(weighting_algo(cur_label))\n",
    "            \n",
    "            # Saving labels and weights\n",
    "            np.save(repeat_id_name,np.array(flipped_label))\n",
    "            np.save(repeat_weight_name,np.array(flipped_weight))\n",
    "    else:\n",
    "        for i in range(len(segmented_labels)):\n",
    "            id_name = id_dst_drc+'id_'+name+'_'+str(i+1)+'.npy'\n",
    "            weight_name = weight_dst_drc+'weight_'+name+'_'+str(i+1)+'.npy'\n",
    "            \n",
    "            # convert image_id into weighted image \n",
    "            cur_label = segmented_labels[i]\n",
    "            weighted_image = weighting_algo(cur_label)\n",
    "\n",
    "            # Saving labels and weights\n",
    "            np.save(id_name,np.array(cur_label))\n",
    "            np.save(weight_name,np.array(weighted_image))\n",
    "\n",
    "    return name\n",
    "\n",
    "# Saving images\n",
    "def save_label(image, src_direct, name):\n",
    "    dst_drc = src_direct\n",
    "    \n",
    "    # Crop all images\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600) # Crop all images at 512,600\n",
    "    elif len(image.shape) == 3:\n",
    "        height, width, colour = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600, colour) # Crop all images at 512,600\n",
    "    id_image = convert_to_id_image(image)\n",
    "    image_name = dst_drc+\"label_\"+name+'.png'\n",
    "    imsave(image_name, id_image)\n",
    "#     rgb_image = convert_to_rgb_image(id_image)\n",
    "#     plt.imshow(id_image)\n",
    "#     plt.imshow(rgb_image)\n",
    "#     print(id_image.shape)\n",
    "#     print(np.unique(id_image))\n",
    "#     return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_files(mydir, ext):\n",
    "    filelist = [ f for f in os.listdir(mydir) if f.endswith(ext) ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))\n",
    "remove = True\n",
    "if remove:\n",
    "    whole_raw_image_folder_rem = whole_raw_image_folder + 'Train/'\n",
    "    brushlet_enhanced_folder_rem = brushlet_enhanced_folder +'Train/'\n",
    "    brushlet_denoised_folder_rem = brushlet_denoised_folder+'Train/'\n",
    "    manual_label_folder_rem = manual_label_folder+'Train/'\n",
    "    \n",
    "    all_labels_folder_ids_rem = all_labels_folder + 'Train/segmented_ids/'\n",
    "    all_labels_folder_weights_rem = all_labels_folder +'Train/weights/'\n",
    "   \n",
    "    sim_labels_folder_ids_rem = sim_labels_folder + 'Train/segmented_ids/'\n",
    "    sim_labels_folder_weights_rem = sim_labels_folder + 'Train/weights/'\n",
    "    \n",
    "    corrected_labels_folder_ids_rem = corrected_labels_folder + 'Train/segmented_ids/'\n",
    "    corrected_labels_folder_weights_rem = corrected_labels_folder + 'Train/weights/'\n",
    "    \n",
    "    normalised_folder_rem = normalised_folder + 'Train/'\n",
    "    \n",
    "#     bm3d_folder_rem = bm3d_folder + 'Train/'\n",
    "#     nlm_folder_rem = nlm_folder + 'Train/'\n",
    "#     bilateral_folder_rem = bilateral_folder + 'Train/'\n",
    "\n",
    "    \n",
    "    remove_files(whole_raw_image_folder_rem, '.png')\n",
    "    remove_files(brushlet_enhanced_folder_rem, '.png')\n",
    "    remove_files(brushlet_denoised_folder_rem, '.png')\n",
    "    remove_files(manual_label_folder_rem, '.png')\n",
    "    remove_files(all_labels_folder_ids_rem, '.npy')\n",
    "    remove_files(all_labels_folder_weights_rem, '.npy')\n",
    "    remove_files(sim_labels_folder_ids_rem, '.npy')\n",
    "    remove_files(sim_labels_folder_weights_rem, '.npy')\n",
    "    remove_files(corrected_labels_folder_ids_rem, '.npy')\n",
    "    remove_files(corrected_labels_folder_weights_rem, '.npy')\n",
    "    remove_files(normalised_folder_rem, '.png')\n",
    "#     remove_files(bm3d_folder_rem, '.png')\n",
    "#     remove_files(nlm_folder_rem, '.png')\n",
    "#     remove_files(bilateral_folder_rem, '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The values of the conversion by Elsa\n",
    "values = [0.0, 0.007843138, 0.011764706, 0.015686275, 0.019607844, 0.023529412, 0.02745098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Extensions\n",
    "raw_ext = '.tif'\n",
    "label_ext = '.JPG'\n",
    "\n",
    "whole_raw_ext = '.tif'\n",
    "man_lab_ext = '.JPG'\n",
    "everything_else_ext = '.png'\n",
    "\n",
    "for i in range(len(files_list[0])):\n",
    "    if i % 2 == 0:\n",
    "        print(i)\n",
    "        whole_raw = plt.imread(whole_raw_image_folder+files_list[list_of_folders.index(whole_raw_image_folder)][i]) # whole raw image - needs to be segmented and saved\n",
    "        \n",
    "        # Labels\n",
    "        man_lab = plt.imread(manual_label_folder+files_list[list_of_folders.index(manual_label_folder)][i]) # main_label (coloured) \n",
    "        correct_label = plt.imread(corrected_labels_folder+'label_'+new_file_list[i]+'_labels.png') # label (ids) \n",
    "        correct_label = ((correct_label*7)/np.max(values)).astype(int)\n",
    "        \n",
    "        # Denoised\n",
    "#         b_d = plt.imread(brushlet_denoised_folder+files_list[list_of_folders.index(brushlet_denoised_folder)][i]) # whole brushlet denoised image \n",
    "#         b_e = plt.imread(brushlet_enhanced_folder+files_list[list_of_folders.index(brushlet_enhanced_folder)][i]) # whole brushlet enhanced image\n",
    "#         bm3d = plt.imread(bm3d_folder+files_list[list_of_folders.index(bm3d_folder)][i]) # bm3d image \n",
    "#         nlm = plt.imread(nlm_folder+files_list[list_of_folders.index(nlm_folder)][i]) # nlm image\n",
    "#         bilateral = plt.imread(bilateral_folder+files_list[list_of_folders.index(bilateral_folder)][i]) # bilateral image \n",
    "        \n",
    "        # Normalised\n",
    "        normalised = plt.imread(normalised_folder+files_list[7][i])\n",
    "        \n",
    "        #### crop and save all files necessary\n",
    "        crop_and_save_raw(whole_raw, whole_raw_image_folder, new_file_list[i])\n",
    "        \n",
    "        crop_and_save_raw(man_lab, manual_label_folder, new_file_list[i])\n",
    "        crop_and_save_label(correct_label, corrected_labels_folder, new_file_list[i])\n",
    "        \n",
    "        # Create my labels from manual labels\n",
    "        # label = plt.imread(all_labels_folder+files_list[4][i]) # label (ids) \n",
    "        # label = ((label*7)/np.max(values)).astype(int)\n",
    "        # save_label(man_lab, sim_labels_folder, new_file_list[i])\n",
    "        # sim_label = plt.imread(sim_labels_folder+files_list[5][i]) # label (ids)\n",
    "        # crop_and_save_label(label, all_labels_folder, new_file_list[i])\n",
    "        # crop_and_save_label(sim_label, sim_labels_folder, new_file_list[i])\n",
    "        \n",
    "        \n",
    "        crop_and_save_raw(normalised, normalised_folder, new_file_list[i])\n",
    "        \n",
    "        \n",
    "#         crop_and_save_raw(b_d, brushlet_denoised_folder, new_file_list[i])\n",
    "#         crop_and_save_raw(b_e, brushlet_enhanced_folder, new_file_list[i])\n",
    "#         crop_and_save_raw(bm3d, bm3d_folder, new_file_list[i])\n",
    "#         crop_and_save_raw(nlm, nlm_folder, new_file_list[i])\n",
    "#         crop_and_save_raw(bilateral, bilateral_folder, new_file_list[i])\n",
    "\n",
    "        # image_array = [b_d, b_e, bm3d, nlm, bilateral]    \n",
    "        # image_name = ['b_d', 'b_e', 'bm3d', 'nlm', 'bilateral']\n",
    "\n",
    "        # fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15,5))\n",
    "        # fig.suptitle(new_file_list[i], size=14)\n",
    "        # for j, ax in enumerate(axes):\n",
    "        #    ax.margins(0.05, 0.15)\n",
    "        #    ax.imshow(image_array[j])\n",
    "        #    ax.set_title(image_name[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(files_list[0][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # for i in range(len(files_list[0])):\n",
    "# for i in range(2):\n",
    "#     if i not in unwanted_files:\n",
    "#         man_lab = plt.imread(manual_label_folder+files_list[3][i])\n",
    "#     #     correct_label = plt.imread(corrected_labels_folder+files_list[6][i])\n",
    "#         correct_label = plt.imread(corrected_labels_folder+'label_'+new_file_list[i]+'_labels.png')\n",
    "#         correct_label = ((correct_label*7)/np.max(values)).astype(int)\n",
    "\n",
    "#         rgb_image = convert_to_rgb_image(correct_label)\n",
    "\n",
    "#         plt.figure(figsize=(15,5))\n",
    "#         plt.imshow(man_lab)\n",
    "#         plt.suptitle(new_file_list[i], size=15)\n",
    "\n",
    "#         print()\n",
    "#         print(new_file_list[i])\n",
    "#         print(np.unique(correct_label))\n",
    "\n",
    "#         plt.figure(figsize=(15,5))\n",
    "#         plt.imshow(correct_label)\n",
    "\n",
    "#         plt.figure(figsize=(15,5))\n",
    "#         plt.imshow(rgb_image)\n",
    "#         plt.show()\n",
    "#         h,w = correct_label.shape\n",
    "#         print(correct_label[400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IMDB of dataset - HP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import h5py\n",
    "# import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# files = list_all_files(whole_raw_image_folder+'Train/')\n",
    "# H = 512\n",
    "# W = 64\n",
    "# N = len(files)\n",
    "# print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # atrium_ids = all_labels_folder+'Train/segmented_ids/'\n",
    "# # atrium_weights = all_labels_folder+'Train/weights/'\n",
    "# atrium_ids = corrected_labels_folder+'Train/segmented_ids/'\n",
    "# atrium_weights = corrected_labels_folder+'Train/weights/'\n",
    "# atrium_raw = whole_raw_image_folder +'Train/'\n",
    "\n",
    "# atrium_id_files = {}\n",
    "# atrium_weights_files = {}\n",
    "# atrium_raw_files = {}\n",
    "# for number, filename in enumerate(sorted(os.listdir(atrium_ids)), start=1):\n",
    "#     atrium_id_files[number] = filename\n",
    "# for number, filename in enumerate(sorted(os.listdir(atrium_weights)), start=1):\n",
    "#     atrium_weights_files[number] = filename\n",
    "# for number, filename in enumerate(sorted(os.listdir(atrium_raw)), start=1):\n",
    "#     atrium_raw_files[number] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load h5py raw array\n",
    "# def make_h5py_array_raw(src_direct, dictionary, count):\n",
    "#     files = []\n",
    "#     for i in range(1,len(dictionary)+1):\n",
    "#         act_file = plt.imread(src_direct+dictionary[i])\n",
    "#         act_file = np.array(act_file)\n",
    "#         files.append(act_file)\n",
    "#         count += 1\n",
    "#     return files, count\n",
    "\n",
    "# # Load h5py py array\n",
    "# def make_h5py_array_npy(src_direct, dictionary, count):\n",
    "#     files = []\n",
    "#     for i in range(1,len(dictionary)+1):\n",
    "#         act_file = np.load(src_direct+dictionary[i])\n",
    "#         files.append(act_file)\n",
    "        \n",
    "#         count += 1\n",
    "#     return files, count\n",
    "# count = 0\n",
    "# id_files,num = make_h5py_array_npy(atrium_ids, atrium_id_files, count)\n",
    "# print(len(id_files),num)\n",
    "# weight_files,num = make_h5py_array_npy(atrium_weights, atrium_weights_files, count)\n",
    "# print(len(weight_files),num)\n",
    "# raw_files, num = make_h5py_array_raw(atrium_raw, atrium_raw_files, count)\n",
    "# print(len(raw_files),num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Creating Data.h5 File\n",
    "# data = np.zeros((N,1, H, W), dtype=np.float32)\n",
    "\n",
    "\n",
    "# for i in range(N):\n",
    "#     image = raw_files[i] # array of size (H,W)\n",
    "#     for m in range(H):\n",
    "#         for n in range(W):\n",
    "#             data[i, 0, m, n] = image[m,n]\n",
    "# data = data.astype('float32')\n",
    "# print(type(data))\n",
    "\n",
    "# # data = torch.from_numpy(data).float()\n",
    "# data = torch.from_numpy(data)\n",
    "# # data = torch.Tensor(data)\n",
    "# print('Finalshape:',data.shape)\n",
    "# hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/Data.h5', 'w')\n",
    "# hf.create_dataset('data', data=data) # creating raw data dataset - group name followed by dimensions in [H,W,Channel,DataIndex]\n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Creating label.h5 File\n",
    "# labels = np.zeros((N,2, H, W), dtype=np.float32)\n",
    "\n",
    "# for i in range(N):\n",
    "#     weights = weight_files[i] # array of size (H,W)\n",
    "#     ids = id_files[i] # array of size (H,W) # class is your colour\n",
    "    \n",
    "#     h,w = ids.shape\n",
    "#     if h != 512 or w != 64:\n",
    "# #         print(h,w)\n",
    "#         amount = H - h\n",
    "#         weight_full = np.full((amount, 64), 1.0)\n",
    "#         id_full = np.full((amount, 64), 7)\n",
    "#         weights = np.concatenate((weights, weight_full))\n",
    "#         ids = np.concatenate((ids, id_full))   \n",
    "#     new_id = np.copy(ids)\n",
    "#     for m in range(H):\n",
    "#         for n in range(W):\n",
    "#             if ids[m,n] > 7:\n",
    "#                 # set all label values to 7 \n",
    "#                 new_id[m,n] = 7\n",
    "#             labels[i, 0, m, n] = new_id[m,n]\n",
    "#             labels[i, 1, m, n] = weights[m,n]\n",
    "            \n",
    "# labels = labels.astype('float32')    \n",
    "# print(type(labels))\n",
    "# # labels = torch.from_numpy(labels).float()\n",
    "# labels = torch.from_numpy(labels)\n",
    "# # print(np.unique(labels[:,0,:,:]))\n",
    "# print('Finalshape:',labels.shape)\n",
    "\n",
    "# hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/label.h5', 'w')\n",
    "# hf.create_dataset('labels', data=labels) \n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Creating set.h5 File\n",
    "# sets = np.ones((1,N))\n",
    "# split = N * 0.8\n",
    "\n",
    "# num_files = N\n",
    "# validation_cutoff = int(split)\n",
    "\n",
    "# # Making training set random\n",
    "# train_indices = np.random.choice(num_files,validation_cutoff,replace = False)\n",
    "# print(train_indices)\n",
    "# test_indices = [x for x in range(num_files) if x not in train_indices]\n",
    "# print(test_indices)\n",
    "# for count,i in enumerate(sets[0]):\n",
    "#     if count not in train_indices:\n",
    "#         sets[0][count] = 3\n",
    "\n",
    "# # sets[:,int(split):] = 3\n",
    "\n",
    "# hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/set.h5', 'w')\n",
    "# hf.create_dataset('set', data=sets)\n",
    "# hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
