{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pprint\n",
    "import cv2\n",
    "from scipy.misc import imsave\n",
    "from helper import *\n",
    "from create_labels import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Applying Weighting algorithm to an image\n",
    "value = 1\n",
    "def weighting_algo(id_image):\n",
    "    '''\n",
    "    Creating weighting of an image\n",
    "    '''\n",
    "    image = id_image\n",
    "    x,y = image.shape\n",
    "    weighted_image = np.zeros((x,y))\n",
    "    for j in range(x):\n",
    "        for k in range(y):\n",
    "            # NOTE: did weights based on bold from legend\n",
    "            if(image[j][k]==1): # 1 red - myocardium\n",
    "                 w2 = 10 * value \n",
    "            elif(image[j][k] == 2): # blue - endocardium\n",
    "                 w2 = 10 * value\n",
    "            elif(image[j][k]== 3): # purple - fibrosis\n",
    "                 w2 = 5 * value \n",
    "            elif(image[j][k] == 4): # green - fat\n",
    "                 w2 = 11 * value\n",
    "            elif(image[j][k]== 5): # orange - dense collagen\n",
    "                 w2 = 5 * value\n",
    "            elif(image[j][k]== 6): # yellow - loose collagen\n",
    "                 w2 = 5 * value\n",
    "            elif(image[j][k]== 7):\n",
    "                 w2 = 8 * value # magenta - smooth muscle\n",
    "            else:\n",
    "                 w2 = 0\n",
    "            if(j!=0 and j!=x-1):\n",
    "                next_pix = image[j+1][k]\n",
    "                prev_pix = image[j-1][k]\n",
    "                # Taking the derivative of the pixels\n",
    "                if(np.absolute((next_pix-prev_pix))>0 and w2!=0):\n",
    "                    w1 = 10  \n",
    "                else:\n",
    "                    w1 = 0\n",
    "            else:\n",
    "                w1 = 0\n",
    "            w = 1 + w1 + w2\n",
    "            weighted_image[j][k] = w\n",
    "    return weighted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 black\n",
    "# 1 red - myocardium\n",
    "# 2 blue - endocardium\n",
    "# 3 purple - fibrosis\n",
    "# 4 green - fat\n",
    "# 5 orange - dense collagen\n",
    "# 6 yellow - loose collagen\n",
    "# 7 magenta - smooth muscle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting the directories\n",
    "import os\n",
    "\n",
    "\n",
    "# wanted_folder = 'alldata/'\n",
    "wanted_folder = 'pruned/'\n",
    "# wanted_folder = 'Atrium/'\n",
    "# wanted_folder = 'Ventricle/'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "check_directory = cwd\n",
    "if check_directory == '/home/sim/notebooks/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets/OCTData/'+wanted_folder\n",
    "elif check_directory == '/Users/sim/Desktop/Imperial/Project/PreTrained/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets-24-aug/OCTData/'+wanted_folder\n",
    "\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whole_raw_image_folder = cwd + 'whole_raw_image/'\n",
    "brushlet_enhanced_folder = cwd + 'brushlet_enhanced/'\n",
    "brushlet_denoised_folder = cwd + 'brushlet_denoised/'\n",
    "manual_label_folder = cwd + 'manual_label/'\n",
    "all_labels_folder = cwd + 'all_labels/'\n",
    "sim_labels_folder = cwd + 'my_labels/'\n",
    "corrected_labels_folder = cwd + 'png_labels_method/'\n",
    "normalised_folder = cwd + 'normalised_raw_image/'\n",
    "# normalised_folder = cwd + 'normalised_raw_2/' # normalising with atrium vs atrium and ventricle vs ventricle\n",
    "\n",
    "list_of_folders = [whole_raw_image_folder, \\\n",
    "                   brushlet_denoised_folder, \\\n",
    "                   brushlet_enhanced_folder, \\\n",
    "                   manual_label_folder, \\\n",
    "                   all_labels_folder, \\\n",
    "                   sim_labels_folder, \\\n",
    "                   corrected_labels_folder, \\\n",
    "                normalised_folder]\n",
    "print(len(list_of_folders))\n",
    "def list_all_files(directory):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    all_labels_files = [f for f in listdir(directory) if isfile(join(directory, f)) and f != '.DS_Store']\n",
    "    all_labels_files.sort()\n",
    "    return all_labels_files\n",
    "\n",
    "files_list = []\n",
    "for folders in list_of_folders:\n",
    "    files_list.append(list_all_files(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(files_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating list of file names\n",
    "new_file_list = []\n",
    "\n",
    "for i in files_list[0]:\n",
    "    if i[:4] == 'con_':\n",
    "        string = i[4:]\n",
    "        file_name = string[:19].replace('.','')\n",
    "    new_file_list.append(file_name)\n",
    "\n",
    "new_file_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing Unwanted Files\n",
    "unwanted_files = []\n",
    "\n",
    "# anon_count = 0\n",
    "# for count, i in enumerate(new_file_list):\n",
    "#     if i == 'H3_N02186_RV_1_270' or i == 'H3_N02186_RV_1_400' or i == 'H4_N03210_LV_1_400' or i == 'H4_N03210_RV_1_390':\n",
    "#         print(count,i)\n",
    "#         unwanted_files.append(count)\n",
    "#     elif i == 'H4_N03210_RV_1_400' or i == 'H5_N03290_LV_1_420' or i == 'H6_N03320_LVS_1_455':\n",
    "#         print(count,i)\n",
    "#         unwanted_files.append(count)\n",
    "# #     Second set of unwanted files if not clear segments\n",
    "#     elif i == 'H1_N01848_LV_1_194' or i == 'H1_N01848_LV_1_300' or i == 'H3_N02186_LVS_1_265':\n",
    "#         print(count,i)\n",
    "#         unwanted_files.append(count)\n",
    "#     elif i == 'H6_N03320_LV_1_404' or i == 'H6_N03320_LVS_1_455':\n",
    "#         print(count,i)\n",
    "#         unwanted_files.append(count)\n",
    "#     elif i == 'H1_N01848_LV_1_194' or i == 'H1_N01848_LV_1_300' or i == 'H2_N02047_LVS_1_400' or i == 'H2_N02047_LVS_1_420' or i == 'H3_N02186_LVS_1_265' or i == 'H4_N03210_RV_1_390' or i == 'H4_N03210_RV_1_400' or i == 'H5_N03290_LV_1_420' or i == 'H6_N03320_LV_1_404' or i == 'H6_N03320_LVS_1_455' or i == 'H6_N03320_RA_1_420' or i == 'H8_N03585_RA_1_345':\n",
    "#         print(count,i)\n",
    "#         unwanted_files.append(count)\n",
    "# #     elif i[11] == 'V':\n",
    "# #         print(count,i)\n",
    "# #         unwanted_files.append(count)\n",
    "print(unwanted_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(unwanted_files))\n",
    "print(len(new_file_list))\n",
    "print(len(new_file_list)-len(unwanted_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Image Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def segment_image_multiple(image, left_bound, right_bound, split):\n",
    "    '''\n",
    "    Segment image at intervals of half of split to have more training data\n",
    "    \n",
    "    Segment image by input of image, left bound and right bound, and split int \n",
    "    '''\n",
    "    list_of_images = []\n",
    "    bounds = right_bound - left_bound\n",
    "    quot, rem = divmod(bounds, split)\n",
    "#     print(quot, rem)\n",
    "    ranged = np.arange(0,quot,0.7)\n",
    "    ranged = ranged[:-1] # change size of range to ensure all have same length\n",
    "    for i in ranged:\n",
    "#         print(i*split, (i+1)*split)\n",
    "        if len(image.shape) == 2:\n",
    "            cropped_image = crop_image(image, int(i*split), int((i+1) * split))\n",
    "            list_of_images.append(cropped_image)\n",
    "        elif len(image.shape) == 3:\n",
    "            cropped_image = crop_image(image, int(i*split), int((i+1) * split), False)\n",
    "            list_of_images.append(cropped_image)\n",
    "#         print(cropped_image.shape)\n",
    "    return list_of_images\n",
    "\n",
    "whole_raw_1 = plt.imread(whole_raw_image_folder+files_list[0][0])\n",
    "segmented_whole = segment_image_multiple(whole_raw_1, 0, 600, 64)\n",
    "\n",
    "plt.imshow(segmented_whole[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving images\n",
    "def crop_and_save_raw(image, src_direct, name):\n",
    "    dst_drc = src_direct + 'Train/'\n",
    "    \n",
    "    # Crop all images\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600) # Crop all images at 512,600\n",
    "    elif len(image.shape) == 3:\n",
    "        height, width, colour = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600, colour) # Crop all images at 512,600\n",
    "#     plt.imshow(image)\n",
    "#     print(image.shape)\n",
    "#     segmented_images = segment_image(image, 0, 600, 64)\n",
    "    segmented_images = segment_image_multiple(image, 0, 600, 64)\n",
    "#     print(len(segmented_images))\n",
    "#     fig, axes = plt.subplots(nrows=1, ncols=len(segmented_images), figsize=(9,20))\n",
    "#     for i, ax in enumerate(axes):\n",
    "#         ax.imshow(segmented_images[i])\n",
    "#         ax.set_title(i)\n",
    "#     plt.show()\n",
    "    for i in range(len(segmented_images)):\n",
    "        image_name = dst_drc+name+'_'+str(i+1)+'.png'\n",
    "        cur_image = segmented_images[i]\n",
    "        imsave(image_name, cur_image)\n",
    "#     return image_name\n",
    "\n",
    "# Saving labels and weights\n",
    "def crop_and_save_label(label, src_direct, name):\n",
    "    id_dst_drc = src_direct + 'Train/segmented_ids/'\n",
    "    weight_dst_drc = src_direct + 'Train/weights/'\n",
    "    height, width = label.shape\n",
    "    \n",
    "    label[440:,:] = 0\n",
    "    \n",
    "    if width > 600:\n",
    "        label = crop_image(label, 0, 600)\n",
    "    if height != 512:\n",
    "        amount = 512 - height\n",
    "        id_full = np.zeros((amount,600))\n",
    "        label = np.concatenate((label, id_full))\n",
    "    \n",
    "    rgb_image = convert_to_rgb_image(label) \n",
    "#     plt.imshow(rgb_image)\n",
    "#     segmented_labels = segment_image(label, 0, 600, 64)\n",
    "    segmented_labels = segment_image_multiple(label, 0, 600, 64)\n",
    "    \n",
    "    for i in range(len(segmented_labels)):\n",
    "        id_name = id_dst_drc+'id_'+name+'_'+str(i+1)+'.npy'\n",
    "        weight_name = weight_dst_drc+'weight_'+name+'_'+str(i+1)+'.npy'\n",
    "        \n",
    "        cur_label = segmented_labels[i]\n",
    "        \n",
    "        # convert image_id into weighted image \n",
    "        weighted_image = weighting_algo(cur_label)\n",
    "        \n",
    "        # Saving labels and weights\n",
    "        np.save(id_name,np.array(cur_label))\n",
    "        np.save(weight_name,np.array(weighted_image))\n",
    "\n",
    "    return name\n",
    "\n",
    "# Saving images\n",
    "def save_label(image, src_direct, name):\n",
    "    dst_drc = src_direct\n",
    "    \n",
    "    # Crop all images\n",
    "    if len(image.shape) == 2:\n",
    "        height, width = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600) # Crop all images at 512,600\n",
    "    elif len(image.shape) == 3:\n",
    "        height, width, colour = image.shape\n",
    "        if width > 600:\n",
    "            image = crop_image(image, 0, 600, colour) # Crop all images at 512,600\n",
    "    id_image = convert_to_id_image(image)\n",
    "    image_name = dst_drc+\"label_\"+name+'.png'\n",
    "    imsave(image_name, id_image)\n",
    "#     rgb_image = convert_to_rgb_image(id_image)\n",
    "#     plt.imshow(id_image)\n",
    "#     plt.imshow(rgb_image)\n",
    "#     print(id_image.shape)\n",
    "#     print(np.unique(id_image))\n",
    "#     return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_files(mydir, ext):\n",
    "    filelist = [ f for f in os.listdir(mydir) if f.endswith(ext) ]\n",
    "    for f in filelist:\n",
    "        os.remove(os.path.join(mydir, f))\n",
    "remove = True\n",
    "if remove:\n",
    "    whole_raw_image_folder_rem = whole_raw_image_folder + 'Train/'\n",
    "#     brushlet_enhanced_folder_rem = brushlet_enhanced_folder +'Train/'\n",
    "#     brushlet_denoised_folder_rem = brushlet_denoised_folder+'Train/'\n",
    "#     manual_label_folder_rem = manual_label_folder+'Train/'\n",
    "    \n",
    "#     all_labels_folder_ids_rem = all_labels_folder + 'Train/segmented_ids/'\n",
    "#     all_labels_folder_weights_rem = all_labels_folder +'Train/weights/'\n",
    "   \n",
    "#     sim_labels_folder_ids_rem = sim_labels_folder + 'Train/segmented_ids/'\n",
    "#     sim_labels_folder_weights_rem = sim_labels_folder + 'Train/weights/'\n",
    "    \n",
    "#     corrected_labels_folder_ids_rem = corrected_labels_folder + 'Train/segmented_ids/'\n",
    "#     corrected_labels_folder_weights_rem = corrected_labels_folder + 'Train/weights/'\n",
    "    \n",
    "    normalised_folder_rem = normalised_folder + 'Train/'\n",
    "    \n",
    "    remove_files(whole_raw_image_folder_rem, '.png')\n",
    "#     remove_files(brushlet_enhanced_folder_rem, '.png')\n",
    "#     remove_files(brushlet_denoised_folder_rem, '.png')\n",
    "#     remove_files(manual_label_folder_rem, '.png')\n",
    "#     remove_files(all_labels_folder_ids_rem, '.npy')\n",
    "#     remove_files(all_labels_folder_weights_rem, '.npy')\n",
    "#     remove_files(sim_labels_folder_ids_rem, '.npy')\n",
    "#     remove_files(sim_labels_folder_weights_rem, '.npy')\n",
    "#     remove_files(corrected_labels_folder_ids_rem, '.npy')\n",
    "#     remove_files(corrected_labels_folder_weights_rem, '.npy')\n",
    "    remove_files(normalised_folder_rem, '.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The values of the conversion by Elsa\n",
    "values = [0.0, 0.007843138, 0.011764706, 0.015686275, 0.019607844, 0.023529412, 0.02745098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extensions\n",
    "raw_ext = '.tif'\n",
    "label_ext = '.JPG'\n",
    "\n",
    "whole_raw_ext = '.tif'\n",
    "man_lab_ext = '.JPG'\n",
    "everything_else_ext = '.png'\n",
    "\n",
    "# Loop through images to determine whether their weights and they are looking correct\n",
    "l_values = set()\n",
    "for i in range(len(files_list[0])):\n",
    "    # choosing files I don't like\n",
    "    if i not in unwanted_files:\n",
    "        print(i)\n",
    "        whole_raw = plt.imread(whole_raw_image_folder+files_list[0][i]) # whole raw image - needs to be segmented and saved\n",
    "#         b_d = plt.imread(brushlet_denoised_folder+files_list[1][i]) # whole brushlet denoised image - needs to be segmented and saved\n",
    "#         b_e = plt.imread(brushlet_enhanced_folder+files_list[2][i]) # whole brushlet enhanced image - needs to be segmented and saved\n",
    "        man_lab = plt.imread(manual_label_folder+files_list[3][i]) # main_label (coloured) - needs to be segmented and saved\n",
    "#         label = plt.imread(all_labels_folder+files_list[4][i]) # label (ids) - needs to be segmented and used to create weight\n",
    "#         label = ((label*7)/np.max(values)).astype(int)\n",
    "        \n",
    "        # Create my labels from manual labels\n",
    "#         save_label(man_lab, sim_labels_folder, new_file_list[i])\n",
    "#         sim_label = plt.imread(sim_labels_folder+files_list[5][i]) # label (ids) - needs to be segmented and used to create weight\n",
    "\n",
    "        correct_label = plt.imread(corrected_labels_folder+'label_'+new_file_list[i]+'_labels.png') # label (ids) - needs to be segmented and used to create weight\n",
    "        correct_label = ((correct_label*7)/np.max(values)).astype(int)\n",
    "        \n",
    "        normalised = plt.imread(normalised_folder+files_list[7][i])\n",
    "        \n",
    "        # crop and save all files necessary\n",
    "        crop_and_save_raw(whole_raw, whole_raw_image_folder, new_file_list[i])\n",
    "#         crop_and_save_raw(b_d, brushlet_denoised_folder, new_file_list[i])\n",
    "#         crop_and_save_raw(b_e, brushlet_enhanced_folder, new_file_list[i])\n",
    "        crop_and_save_raw(man_lab, manual_label_folder, new_file_list[i])\n",
    "#         crop_and_save_label(label, all_labels_folder, new_file_list[i])\n",
    "#         crop_and_save_label(sim_label, sim_labels_folder, new_file_list[i])\n",
    "        crop_and_save_label(correct_label, corrected_labels_folder, new_file_list[i])\n",
    "    \n",
    "        crop_and_save_raw(normalised, normalised_folder, new_file_list[i])\n",
    "\n",
    "#         image_array = [whole_raw, b_d, b_e, man_lab, label]    \n",
    "#         image_name = ['whole_raw_1', 'b_d_1', 'b_e_1', 'man_lab_1', 'label_1']\n",
    "\n",
    "#         fig, axes = plt.subplots(nrows=1, ncols=5, figsize=(15,5))\n",
    "#         fig.suptitle(new_file_list[i], size=14)\n",
    "#         for j, ax in enumerate(axes):\n",
    "#             ax.margins(0.05, 0.15)\n",
    "#             ax.imshow(image_array[j])\n",
    "#             ax.set_title(image_name[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(files_list[0][15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # for i in range(len(files_list[0])):\n",
    "# for i in range(2):\n",
    "#     if i not in unwanted_files:\n",
    "#         man_lab = plt.imread(manual_label_folder+files_list[3][i])\n",
    "#     #     correct_label = plt.imread(corrected_labels_folder+files_list[6][i])\n",
    "#         correct_label = plt.imread(corrected_labels_folder+'label_'+new_file_list[i]+'_labels.png')\n",
    "#         correct_label = ((correct_label*7)/np.max(values)).astype(int)\n",
    "\n",
    "#         rgb_image = convert_to_rgb_image(correct_label)\n",
    "\n",
    "#         plt.figure(figsize=(15,5))\n",
    "#         plt.imshow(man_lab)\n",
    "#         plt.suptitle(new_file_list[i], size=15)\n",
    "\n",
    "#         print()\n",
    "#         print(new_file_list[i])\n",
    "#         print(np.unique(correct_label))\n",
    "\n",
    "#         plt.figure(figsize=(15,5))\n",
    "#         plt.imshow(correct_label)\n",
    "\n",
    "#         plt.figure(figsize=(15,5))\n",
    "#         plt.imshow(rgb_image)\n",
    "#         plt.show()\n",
    "#         h,w = correct_label.shape\n",
    "#         print(correct_label[400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IMDB of dataset - HP5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "files = list_all_files(whole_raw_image_folder+'Train/')\n",
    "H = 512\n",
    "W = 64\n",
    "N = len(files)\n",
    "print(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# atrium_ids = all_labels_folder+'Train/segmented_ids/'\n",
    "# atrium_weights = all_labels_folder+'Train/weights/'\n",
    "atrium_ids = corrected_labels_folder+'Train/segmented_ids/'\n",
    "atrium_weights = corrected_labels_folder+'Train/weights/'\n",
    "atrium_raw = whole_raw_image_folder +'Train/'\n",
    "\n",
    "atrium_id_files = {}\n",
    "atrium_weights_files = {}\n",
    "atrium_raw_files = {}\n",
    "for number, filename in enumerate(sorted(os.listdir(atrium_ids)), start=1):\n",
    "    atrium_id_files[number] = filename\n",
    "for number, filename in enumerate(sorted(os.listdir(atrium_weights)), start=1):\n",
    "    atrium_weights_files[number] = filename\n",
    "for number, filename in enumerate(sorted(os.listdir(atrium_raw)), start=1):\n",
    "    atrium_raw_files[number] = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load h5py raw array\n",
    "def make_h5py_array_raw(src_direct, dictionary, count):\n",
    "    files = []\n",
    "    for i in range(1,len(dictionary)+1):\n",
    "        act_file = plt.imread(src_direct+dictionary[i])\n",
    "        act_file = np.array(act_file)\n",
    "        files.append(act_file)\n",
    "        count += 1\n",
    "    return files, count\n",
    "\n",
    "# Load h5py py array\n",
    "def make_h5py_array_npy(src_direct, dictionary, count):\n",
    "    files = []\n",
    "    for i in range(1,len(dictionary)+1):\n",
    "        act_file = np.load(src_direct+dictionary[i])\n",
    "        files.append(act_file)\n",
    "        \n",
    "        count += 1\n",
    "    return files, count\n",
    "count = 0\n",
    "id_files,num = make_h5py_array_npy(atrium_ids, atrium_id_files, count)\n",
    "print(len(id_files),num)\n",
    "weight_files,num = make_h5py_array_npy(atrium_weights, atrium_weights_files, count)\n",
    "print(len(weight_files),num)\n",
    "raw_files, num = make_h5py_array_raw(atrium_raw, atrium_raw_files, count)\n",
    "print(len(raw_files),num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Creating Data.h5 File\n",
    "data = np.zeros((N,1, H, W), dtype=np.float32)\n",
    "\n",
    "\n",
    "for i in range(N):\n",
    "    image = raw_files[i] # array of size (H,W)\n",
    "    for m in range(H):\n",
    "        for n in range(W):\n",
    "            data[i, 0, m, n] = image[m,n]\n",
    "data = data.astype('float32')\n",
    "print(type(data))\n",
    "\n",
    "# data = torch.from_numpy(data).float()\n",
    "data = torch.from_numpy(data)\n",
    "# data = torch.Tensor(data)\n",
    "print('Finalshape:',data.shape)\n",
    "hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/Data.h5', 'w')\n",
    "hf.create_dataset('data', data=data) # creating raw data dataset - group name followed by dimensions in [H,W,Channel,DataIndex]\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating label.h5 File\n",
    "labels = np.zeros((N,2, H, W), dtype=np.float32)\n",
    "\n",
    "for i in range(N):\n",
    "    weights = weight_files[i] # array of size (H,W)\n",
    "    ids = id_files[i] # array of size (H,W) # class is your colour\n",
    "    \n",
    "    h,w = ids.shape\n",
    "    if h != 512 or w != 64:\n",
    "#         print(h,w)\n",
    "        amount = H - h\n",
    "        weight_full = np.full((amount, 64), 1.0)\n",
    "        id_full = np.full((amount, 64), 7)\n",
    "        weights = np.concatenate((weights, weight_full))\n",
    "        ids = np.concatenate((ids, id_full))   \n",
    "    new_id = np.copy(ids)\n",
    "    for m in range(H):\n",
    "        for n in range(W):\n",
    "            if ids[m,n] > 7:\n",
    "                # set all label values to 7 \n",
    "                new_id[m,n] = 7\n",
    "            labels[i, 0, m, n] = new_id[m,n]\n",
    "            labels[i, 1, m, n] = weights[m,n]\n",
    "            \n",
    "labels = labels.astype('float32')    \n",
    "print(type(labels))\n",
    "# labels = torch.from_numpy(labels).float()\n",
    "labels = torch.from_numpy(labels)\n",
    "# print(np.unique(labels[:,0,:,:]))\n",
    "print('Finalshape:',labels.shape)\n",
    "\n",
    "hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/label.h5', 'w')\n",
    "hf.create_dataset('labels', data=labels) \n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating set.h5 File\n",
    "sets = np.ones((1,N))\n",
    "split = N * 0.8\n",
    "\n",
    "num_files = N\n",
    "validation_cutoff = int(split)\n",
    "\n",
    "# Making training set random\n",
    "train_indices = np.random.choice(num_files,validation_cutoff,replace = False)\n",
    "print(train_indices)\n",
    "test_indices = [x for x in range(num_files) if x not in train_indices]\n",
    "print(test_indices)\n",
    "for count,i in enumerate(sets[0]):\n",
    "    if count not in train_indices:\n",
    "        sets[0][count] = 3\n",
    "\n",
    "# sets[:,int(split):] = 3\n",
    "\n",
    "hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/set.h5', 'w')\n",
    "hf.create_dataset('set', data=sets)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# N = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Creating Data.h5 File\n",
    "# data = np.zeros((N,1, H, W), dtype=np.float32)\n",
    "\n",
    "\n",
    "# for i in range(N):\n",
    "#     image = raw_files[i] # array of size (H,W)\n",
    "#     for m in range(H):\n",
    "#         for n in range(W):\n",
    "#             data[i, 0, m, n] = image[m,n]\n",
    "# data = data.astype('float32')\n",
    "# print(type(data))\n",
    "\n",
    "# # data = torch.from_numpy(data).float()\n",
    "# data = torch.from_numpy(data)\n",
    "# # # data = torch.Tensor(data)\n",
    "# print('Finalshape:',data.shape)\n",
    "# hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/Data.h5', 'w')\n",
    "# hf.create_dataset('data', data=data) # creating raw data dataset - group name followed by dimensions in [H,W,Channel,DataIndex]\n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating label.h5 File\n",
    "# labels = np.zeros((N, 2, H, W), dtype=np.float32)\n",
    "\n",
    "# for i in range(N):\n",
    "#     weights = weight_files[i] # array of size (H,W)\n",
    "#     ids = id_files[i] # array of size (H,W) # class is your colour\n",
    "    \n",
    "#     h,w = ids.shape\n",
    "#     if h != 512 or w != 64:\n",
    "# #         print(h,w)\n",
    "#         amount = H - h\n",
    "#         weight_full = np.full((amount, 64), 1.0)\n",
    "#         id_full = np.full((amount, 64), 7)\n",
    "#         weights = np.concatenate((weights, weight_full))\n",
    "#         ids = np.concatenate((ids, id_full))   \n",
    "#     new_id = np.copy(ids)\n",
    "#     print(np.unique(new_id))\n",
    "#     for m in range(H):\n",
    "#         for n in range(W):\n",
    "#             if ids[m,n] > 7:\n",
    "#                 # set all label values to 7 \n",
    "#                 new_id[m,n] = 7\n",
    "#             labels[i, 0, m, n] = new_id[m,n]\n",
    "#             labels[i, 1, m, n] = weights[m,n]\n",
    "# #     print(np.unique(labels))\n",
    "# labels = labels.astype('float32')    \n",
    "# print(type(labels))\n",
    "# # labels = torch.from_numpy(labels).float()\n",
    "# labels = torch.from_numpy(labels)\n",
    "# print(np.unique(labels[:,0,:,:]))\n",
    "# print('Finalshape:',labels.shape)\n",
    "\n",
    "# hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/label.h5', 'w')\n",
    "# hf.create_dataset('labels', data=labels) \n",
    "# hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Creating set.h5 File\n",
    "# sets = np.ones((1,N))\n",
    "# split = N * 0.8\n",
    "# sets[:,int(split):] = 3\n",
    "\n",
    "# hf = h5py.File('/home/sim/notebooks/relaynet_pytorch/datasets/OCTData/set.h5', 'w')\n",
    "# hf.create_dataset('set', data=sets)\n",
    "# hf.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
