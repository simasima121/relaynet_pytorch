{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np    \n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import pprint\n",
    "import cv2\n",
    "from scipy.misc import imsave\n",
    "from helper import *\n",
    "from create_labels import label_img_to_rgb\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "import keras\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import Lambda \n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import History \n",
    "history = History()\n",
    "\n",
    "from keras.layers import Reshape\n",
    "\n",
    "from keras import backend as K\n",
    "from keras import regularizers, optimizers\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.2)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "\n",
    "# sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h,w = 512, 600\n",
    "data_shape = h*w\n",
    "weight_decay = 0.0001\n",
    "# Defines the input tensor\n",
    "inputs = Input(shape=(h,w,1))\n",
    "k,s = 5,5\n",
    "L1 = Conv2D(64,kernel_size=(k,s),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(inputs)\n",
    "L2 = BatchNormalization()(L1)\n",
    "L2 = Activation('relu')(L2)\n",
    "#L3 = Lambda(maxpool_1,output_shape = shape)(L2)\n",
    "L3 = MaxPooling2D(pool_size=(2,2))(L2)\n",
    "L4 = Conv2D(64,kernel_size=(k,s),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L3)\n",
    "L5 = BatchNormalization()(L4)\n",
    "L5 = Activation('relu')(L5)\n",
    "#L6 = Lambda(maxpool_2,output_shape = shape)(L5)\n",
    "L6 = MaxPooling2D(pool_size=(2,2))(L5)\n",
    "L7 = Conv2D(64,kernel_size=(k,s),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L6)\n",
    "L8 = BatchNormalization()(L7)\n",
    "L8 = Activation('relu')(L8)\n",
    "#L9 = Lambda(maxpool_3,output_shape = shape)(L8)\n",
    "L9 = MaxPooling2D(pool_size=(2,2))(L8)\n",
    "L10 = Conv2D(64,kernel_size=(k,s),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L9)\n",
    "L11 = BatchNormalization()(L10)\n",
    "L11 = Activation('relu')(L11)\n",
    "L12 = UpSampling2D(size = (2,2))(L11)\n",
    "#L12 = Lambda(unpool_3,output_shape = unpool_shape)(L11)\n",
    "L13 = Concatenate(axis = 3)([L8,L12])\n",
    "L14 = Conv2D(64,kernel_size=(k,s),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L13)\n",
    "L15 = BatchNormalization()(L14)\n",
    "L15 = Activation('relu')(L15)\n",
    "L16 = UpSampling2D(size= (2,2))(L15)\n",
    "#L16 = Lambda(unpool_2,output_shape=unpool_shape)(L15)\n",
    "L17 = Concatenate(axis = 3)([L16,L5])\n",
    "L18 = Conv2D(64,kernel_size=(k,s),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L17)\n",
    "L19 = BatchNormalization()(L18)\n",
    "L19 = Activation('relu')(L19)\n",
    "#L20 = Lambda(unpool_1,output_shape=unpool_shape)(L19)\n",
    "L20 = UpSampling2D(size=(2,2),name = \"Layer19\")(L19)\n",
    "L21 = Concatenate(axis=3)([L20,L2])\n",
    "L22 = Conv2D(64,kernel_size=(k,s),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L21)\n",
    "L23 = BatchNormalization()(L22)\n",
    "L23 = Activation('relu')(L23)\n",
    "L24 = Conv2D(8,kernel_size=(1,1),padding = \"same\",kernel_regularizer=regularizers.l2(weight_decay))(L23)\n",
    "L = Reshape((data_shape,8),input_shape = (h,w,8))(L24)\n",
    "L = Activation('softmax')(L)\n",
    "model = Model(inputs = inputs, outputs = L)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Setting the directories\n",
    "import os\n",
    "\n",
    "# wanted_folder = 'alldata/'\n",
    "wanted_folder = 'pruned/'\n",
    "# wanted_folder = 'Atrium/'\n",
    "# wanted_folder = 'Ventricle/'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "check_directory = cwd\n",
    "results_folder = cwd + '/results'\n",
    "if check_directory == '/home/sim/notebooks/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets/OCTData/'+wanted_folder\n",
    "elif check_directory == '/Users/sim/Desktop/Imperial/Project/PreTrained/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets-24-aug/OCTData/'+wanted_folder\n",
    "\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(k,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalised Result\n",
    "# result_sub_folder = \"/Normalised/\"\n",
    "# saved_name = 'raw_bs_40_ep_150'\n",
    "# filenames, raw_images = get_data(cwd+'whole_raw_image','.tif')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'raw'+ '.npy'\n",
    "# saved_name = 'normalised_bs_40_ep_150'\n",
    "# filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'normalised'+ '.npy'\n",
    "\n",
    "# Pruned\n",
    "# result_sub_folder = \"/Pruned/\"\n",
    "# saved_name = 'pruned_raw_bs_40_ep_200'\n",
    "# filenames, raw_images = get_data(cwd+'whole_raw_image','.tif')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'raw'+ '.npy'\n",
    "# saved_name = 'pruned_normalised_bs_40_ep_200'\n",
    "# filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'normalised'+ '.npy'\n",
    "\n",
    "# KS\n",
    "# result_sub_folder = \"/Kernel/\"\n",
    "# saved_name = 'ks55_bs_40_ep_200'\n",
    "# filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'ks_55'+ '.npy'\n",
    "# saved_name = 'ks44_bs_40_ep_150'\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'ks_44'+ '.npy'\n",
    "\n",
    "# Weights\n",
    "# result_sub_folder = \"/Weights/\"\n",
    "# saved_name = 'p4r8g11mag6bs_40_ep_150'\n",
    "# filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'weight'+ '.npy'\n",
    "\n",
    "# LR\n",
    "# result_sub_folder = \"/LR/\"\n",
    "# saved_name = 'lr002_bs_40_ep_150'\n",
    "# filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'lr_002'+ '.npy'\n",
    "# saved_name = 'lr0025_bs_40_ep_150'\n",
    "# filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'lr_0025'+ '.npy'\n",
    "\n",
    "# Epochs\n",
    "result_sub_folder = \"/Epochs/\"\n",
    "# saved_name = 'normalised_bs_40_ep_80'\n",
    "# filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "# save_name = results_folder + result_sub_folder + 'tf_' + 'normalised_bs_40_ep_80'+ '.npy'\n",
    "saved_name = 'normalised_bs_40_ep_100'\n",
    "filenames, raw_images = get_data(cwd+'normalised_raw_image','.png')\n",
    "save_name = results_folder + result_sub_folder + 'tf_' + 'normalised_bs_40_ep_100'+ '.npy'\n",
    "\n",
    "\n",
    "mod = \"/models\"+result_sub_folder+saved_name+\".hdf5\"\n",
    "print(mod, save_name)\n",
    "x = model.load_weights(check_directory+mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stored_images = []\n",
    "for i in range(len(raw_images)):\n",
    "        testing_image = raw_images[i]\n",
    "        h,w = testing_image.shape\n",
    "        testing_image = testing_image.reshape((1,h,w,1))\n",
    "        prediction = model.predict(testing_image)\n",
    "        prediction = np.squeeze(prediction,axis = 0)\n",
    "        prediction = np.reshape(prediction,(h,w,8))\n",
    "        predicted_id = np.zeros((h,w))\n",
    "        for k in range(h):\n",
    "            for j in range(w):\n",
    "                index = np.argmax(prediction[k][j]) # doing pixel wise prediction based on highest prob of class\n",
    "                predicted_id[k][j] = index\n",
    "\n",
    "        stored_images.append(predicted_id)\n",
    "        if i % 2 != 0:\n",
    "            axis_name = filenames[i][4:-4]\n",
    "            idxs = label_img_to_rgb(predicted_id)\n",
    "            plt.imshow(idxs)\n",
    "            plt.show()\n",
    "stored_images = np.asarray(stored_images)\n",
    "\n",
    "np.save(save_name,stored_images)\n",
    "# idxs = label_img_to_rgb(out)\n",
    "# plt.imshow(idxs)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(stored_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 1\n",
    "\n",
    "testing_image = raw_images[i]\n",
    "h,w = testing_image.shape\n",
    "testing_image = testing_image.reshape((1,h,w,1))\n",
    "prediction = model.predict(testing_image)\n",
    "prediction = np.squeeze(prediction,axis = 0)\n",
    "prediction = np.reshape(prediction,(h,w,8))\n",
    "predicted_id = np.zeros((h,w))\n",
    "for k in range(h):\n",
    "    for j in range(w):\n",
    "        index = np.argmax(prediction[k][j]) # doing pixel wise prediction based on highest prob of class\n",
    "        predicted_id[k][j] = index\n",
    "\n",
    "idxs = label_img_to_rgb(predicted_id)\n",
    "plt.imshow(idxs)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF GPU",
   "language": "python",
   "name": "gputf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
