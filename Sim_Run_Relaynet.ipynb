{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "# import torch.onnx\n",
    "\n",
    "from relaynet_pytorch.relay_net import ReLayNet\n",
    "from relaynet_pytorch.data_utils import get_imdb_data\n",
    "\n",
    "# from networks.relay_net import ReLayNet\n",
    "#from networks.data_utils import get_imdb_data\n",
    "\n",
    "#torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Model\n",
    "\n",
    "First Line below removes warnings from ReLayNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture --no-stdout \n",
    "## My adapted code\n",
    "from torch.utils.serialization import load_lua\n",
    "model = cwd + '/models/Exp01/relaynet_epoch20.model'\n",
    "# model = cwd + '/models/relaynet_good.model'\n",
    "\n",
    "# load the model\n",
    "relaynet_model = torch.load(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Printing the test data and the weights for the layers out\n",
    "# relaynet_model # architecture of the net\n",
    "# relaynet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list(relaynet_model.encode1.parameters()) # weights on certain layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Printing model weights\n",
    "# w = list(relaynet_model.parameters())\n",
    "# w\n",
    "# for param in relaynet_model.parameters():\n",
    "#   print(param.data)\n",
    "# list(relaynet_model.parameters()) # all weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(torch.__version__)\n",
    "# torch.cuda.FloatTensor(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# file = '/datasets/ResizedImages'\n",
    "file = '/datasets/DenoiseImages/DenoisedTrainImages/denoised_5.png'\n",
    "# file = '/datasets/con_H1_N01848_LV_1_194.tif'\n",
    "# file = '/datasets/con_H1_N01848_LA_1_272.tif'\n",
    "# file = '/datasets/con_H1_N01848_LA_1_272_denoised.png'\n",
    "# file = '/datasets/test_image.png'\n",
    "\n",
    "directory = cwd + file\n",
    "\n",
    "# # Seeing whether file exists\n",
    "image = plt.imread(directory)\n",
    "\n",
    "test_data = image\n",
    "test_data.shape\n",
    "# # test_data = test_data[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_data,cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All divisors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def divisorGenerator(n):\n",
    "    large_divisors = []\n",
    "    for i in range(1, int(math.sqrt(n) + 1)):\n",
    "        if n % i == 0:\n",
    "            yield i\n",
    "            if i*i != n:\n",
    "                large_divisors.append(n / i)\n",
    "    for divisor in reversed(large_divisors):\n",
    "        yield divisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coloured Labelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SEG_LABELS_LIST = [\n",
    "    {\"id\": -1, \"name\": \"void\", \"rgb_values\": [0, 0, 0]},\n",
    "    {\"id\": 0, \"name\": \"Region above the retina (RaR)\", \"rgb_values\": [128, 0, 0]},\n",
    "    {\"id\": 1, \"name\": \"ILM: Inner limiting membrane\", \"rgb_values\": [0, 128, 0]},\n",
    "    {\"id\": 2, \"name\": \"NFL-IPL: Nerve fiber ending to Inner plexiform layer\", \"rgb_values\": [128, 128, 0]},\n",
    "    {\"id\": 3, \"name\": \"INL: Inner Nuclear layer\", \"rgb_values\": [0, 0, 128]},\n",
    "    {\"id\": 4, \"name\": \"OPL: Outer plexiform layer\", \"rgb_values\": [128, 0, 128]},\n",
    "    {\"id\": 5, \"name\": \"ONL-ISM: Outer Nuclear layer to Inner segment myeloid\", \"rgb_values\": [0, 128, 128]},\n",
    "    {\"id\": 6, \"name\": \"ISE: Inner segment ellipsoid\", \"rgb_values\": [128, 128, 128]},\n",
    "    {\"id\": 7, \"name\": \"OS-RPE: Outer segment to Retinal pigment epithelium\", \"rgb_values\": [64, 0, 0]},\n",
    "    {\"id\": 8, \"name\": \"Region below RPE (RbR)\", \"rgb_values\": [192, 0, 0]}];\n",
    "    #{\"id\": 9, \"name\": \"Fluid region\", \"rgb_values\": [64, 128, 0]}];\n",
    "    \n",
    "def label_img_to_rgb(label_img):\n",
    "    label_img = np.squeeze(label_img)\n",
    "    labels = np.unique(label_img)\n",
    "    label_infos = [l for l in SEG_LABELS_LIST if l['id'] in labels]\n",
    "\n",
    "    label_img_rgb = np.array([label_img,\n",
    "                              label_img,\n",
    "                              label_img]).transpose(1,2,0)\n",
    "    for l in label_infos:\n",
    "        mask = label_img == l['id']\n",
    "        label_img_rgb[mask] = l['rgb_values']\n",
    "\n",
    "    return label_img_rgb.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def show_image(idxs):\n",
    "    idxs = idxs.data.cpu().numpy()\n",
    "    idxs = label_img_to_rgb(idxs)\n",
    "    plt.imshow(idxs)\n",
    "    plt.show()\n",
    "\n",
    "def show_main_image(img_data):\n",
    "    img_data = np.squeeze(img_data) # we do squeeze image test to remove any additional unwanted dimensions.\n",
    "    plt.imshow(img_data)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O/P of ReLayNet Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relay_out(test_data):\n",
    "    out = relaynet_model(Variable(torch.cuda.FloatTensor(test_data)))\n",
    "    out = F.softmax(out,dim=1)\n",
    "    max_val, idx = torch.max(out,1) # torch.max(input, dim, keepdim=False, out=None) -> (Tensor, LongTensor)\n",
    "    return max_val, idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sliding window function to segment certain parts of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source: https://web.archive.org/web/20170223091206/http://www.johnvinyard.com/blog/?p=268\n",
    "# Source: https://stackoverflow.com/questions/22685274/divide-an-image-into-5x5-blocks-in-python-and-compute-histogram-for-each-block\n",
    "from numpy.lib.stride_tricks import as_strided as ast\n",
    "from itertools import product\n",
    "\n",
    "def norm_shape(shape):\n",
    "    '''\n",
    "    Normalize numpy array shapes so they're always expressed as a tuple, \n",
    "    even for one-dimensional shapes.\n",
    "\n",
    "    Parameters\n",
    "        shape - an int, or a tuple of ints\n",
    "\n",
    "    Returns\n",
    "        a shape tuple\n",
    "    '''\n",
    "    try:\n",
    "        i = int(shape)\n",
    "        return (i,)\n",
    "    except TypeError:\n",
    "        # shape was not a number\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        t = tuple(shape)\n",
    "        return t\n",
    "    except TypeError:\n",
    "        # shape was not iterable\n",
    "        pass\n",
    "\n",
    "    raise TypeError('shape must be an int, or a tuple of ints')\n",
    "\n",
    "def sliding_window(a,ws,ss = None,flatten = False):\n",
    "    '''\n",
    "    Return a sliding window over a in any number of dimensions\n",
    "     \n",
    "    Parameters:\n",
    "        a  - an n-dimensional numpy array\n",
    "        ws - an int (a is 1D) or tuple (a is 2D or greater) representing the size \n",
    "             of each dimension of the window\n",
    "        ss - an int (a is 1D) or tuple (a is 2D or greater) representing the \n",
    "             amount to slide the window in each dimension. If not specified, it\n",
    "             defaults to ws.\n",
    "        flatten - if True, all slices are flattened, otherwise, there is an \n",
    "                  extra dimension for each dimension of the input.\n",
    "     \n",
    "    Returns\n",
    "        an array containing each n-dimensional window from a\n",
    "    '''\n",
    "     \n",
    "    if None is ss:\n",
    "        # ss was not provided. the windows will not overlap in any direction.\n",
    "        ss = ws\n",
    "    ws = norm_shape(ws)\n",
    "    ss = norm_shape(ss)\n",
    "     \n",
    "    # convert ws, ss, and a.shape to numpy arrays so that we can do math in every \n",
    "    # dimension at once.\n",
    "    ws = np.array(ws)\n",
    "    ss = np.array(ss)\n",
    "    shape = np.array(a.shape)\n",
    "     \n",
    "     \n",
    "    # ensure that ws, ss, and a.shape all have the same number of dimensions\n",
    "    ls = [len(shape),len(ws),len(ss)]\n",
    "    if 1 != len(set(ls)):\n",
    "        raise ValueError(\\\n",
    "        'a.shape, ws and ss must all have the same length. They were %s' % str(ls))\n",
    "     \n",
    "    # ensure that ws is smaller than a in every dimension\n",
    "    if np.any(ws > shape):\n",
    "        raise ValueError(\\\n",
    "        'ws cannot be larger than a in any dimension.\\\n",
    " a.shape was %s and ws was %s' % (str(a.shape),str(ws)))\n",
    "     \n",
    "    # how many slices will there be in each dimension?\n",
    "    newshape = norm_shape(((shape - ws) // ss) + 1)\n",
    "    # the shape of the strided array will be the number of slices in each dimension\n",
    "    # plus the shape of the window (tuple addition)\n",
    "    newshape += norm_shape(ws)\n",
    "    # the strides tuple will be the array's strides multiplied by step size, plus\n",
    "    # the array's strides (tuple addition)\n",
    "    newstrides = norm_shape(np.array(a.strides) * ss) + a.strides\n",
    "    strided = ast(a,shape = newshape,strides = newstrides)\n",
    "    if not flatten:\n",
    "        return strided\n",
    "     \n",
    "    # Collapse strided so that it has one more dimension than the window.  I.e.,\n",
    "    # the new array is a flat list of slices.\n",
    "    meat = len(ws) if ws.shape else 0\n",
    "    firstdim = (np.product(newshape[:-meat]),) if ws.shape else ()\n",
    "    dim = firstdim + (newshape[-meat:])\n",
    "    # remove any dimensions with size 1\n",
    "#     dim = filter(lambda i : i != 1,dim)\n",
    "    dim = list(filter(lambda i : i != 1,dim))\n",
    "    return strided.reshape(dim)\n",
    "\n",
    "# How to use sliding window function above\n",
    "# rows = 512\n",
    "# columns = 600\n",
    "# divisor = 54\n",
    "# col_size, col_overlap = divmod(columns, divisor)\n",
    "# row_size, row_overlap = divmod(rows, divisor)\n",
    "# ws = (row_size, col_size)\n",
    "# ss = (row_size - row_overlap, col_size - col_overlap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def segment(test_data, x, y):\n",
    "    '''\n",
    "    Takes data, width and height and returns list of images that can be classified individually\n",
    "    \n",
    "    X dimension has to be multiples of 8 because sizes of tensors must match except in dimension 1.\n",
    "    '''\n",
    "    seg_images_list = [] # list of segmented images\n",
    "    x_init_val = 0\n",
    "    y_init_val = 0\n",
    "    height, width = test_data.shape\n",
    "#     print(x_init_val)\n",
    "#     print(height,width)\n",
    "    for i in range(x,width,x):\n",
    "#         print(\"in loop\")\n",
    "#         print(\"Initial x: {}, i:{}, x:{}\".format(x_init_val, i, x))\n",
    "        segmented_data = test_data[:,x_init_val:i] # split data\n",
    "        segmented_data.shape = (1,1,y,x)\n",
    "        seg_images_list.append(segmented_data)\n",
    "        x_init_val += x\n",
    "#     for i, j in zip(range(x,width,x), range(y,height,y)):\n",
    "#         print(\"in loop\")\n",
    "#         print(\"Initial y: {}, j:{}, y:{}\".format(y_init_val, j, y))\n",
    "#         print(\"Initial x: {}, i:{}, x:{}\".format(x_init_val, i, x))\n",
    "#         segmented_data = test_data[y_init_val:y, x_init_val:i] # split data \n",
    "#         segmented_data.shape = (1,1,y,x)\n",
    "#         seg_images_list.append(segmented_data)\n",
    "#         x_init_val += x\n",
    "#         y_init_val += x\n",
    "#     out_seg = relay_out(new_test)\n",
    "    \n",
    "    return segmented_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Testing data Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X = np.zeros((512,600)) # create numpy array of size 512 x 600\n",
    "# data = np.array([[11, 22, 33, 44, 55],[2,3,4,5,6],[10,20,30,40,50]])\n",
    "# print(data[:,0])\n",
    "# print(type(test_data))\n",
    "# print(test_data.shape)\n",
    "# print(test_data.dtype)\n",
    "# new_test_data = np.zeros((512,600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAABoCAYAAADVY3WvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dummy Input\n",
    "# Input is organised [No., Channel, Width, Height] - [1,1,64,512]\n",
    "# Float tensor because weights are float Tensors\n",
    "\n",
    "dummy_input = Variable(torch.cuda.FloatTensor(1,1,64,512)) # has to be (1,1,...) because the first is no. therefore can't plot 2d image on same plot.\n",
    "# print(dummy_input)\n",
    "# Getting ReLayNet max_val, idx classification values\n",
    "max_val_trans, idx_trans = relay_out(dummy_input)\n",
    "\n",
    "# Test image\n",
    "show_image(idx_trans)\n",
    "\n",
    "# new_test = np.transpose(new_test, (0, 1, 3, 2)) # Transposing changes rotation of image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Size Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_test = np.copy(test_data) # Copy so it doesn't alter init dimensions of test_data\n",
    "# seg_test.shape = (1,1,512,600) # Change dimensions of data\n",
    "\n",
    "# new_test = np.transpose(new_test, (0, 1, 3, 2)) # Transposing - changes rotation of image\n",
    "\n",
    "# Getting ReLayNet max_val, idx classification values\n",
    "# max_val_trans, idx_trans = relay_out(new_test)\n",
    "\n",
    "# Test image\n",
    "# show_image(idx_trans)\n",
    "\n",
    "# Main Image\n",
    "# img_test = np.transpose(test_data, (1,0)) # transpose as changes direction of image \n",
    "# show_main_image(img_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # How to use sliding window function above\n",
    "# height, width = test_data.shape\n",
    "# rows = height # height\n",
    "# columns = width # width\n",
    "# divisor = 4\n",
    "# col_size, col_overlap = divmod(columns, divisor)\n",
    "# row_size, row_overlap = divmod(rows, divisor)\n",
    "# ws = (row_size, col_size)\n",
    "# ss = (row_size - row_overlap, col_size - col_overlap)\n",
    "# # calling sliding window function to return \n",
    "# arrays = sliding_window(test_data, ws, ss)\n",
    "# print(ws,ss)\n",
    "# a1 = arrays[1]\n",
    "# print(a1.shape)\n",
    "# byte, h1,w1 = a1.shape\n",
    "# rows = h1 # height\n",
    "# columns = w1 # width\n",
    "# divisor = 2\n",
    "# col_size, col_overlap = divmod(columns, divisor)\n",
    "# row_size, row_overlap = divmod(rows, divisor)\n",
    "# ws = (row_size, col_size)\n",
    "# ss = (row_size - row_overlap, col_size - col_overlap)\n",
    "# print(ws,ss)\n",
    "# a1.reshape(1,1,64,75)\n",
    "# a1.shape = (1,1,h1,w1)\n",
    "# print(a1.shape)\n",
    "# out_eg = relay_out(a1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose vs Not Transpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FaceLandmarksDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "#         self.landmarks_frame = pd.read_csv(csv_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:].as_matrix()\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 512, 600])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n",
      "torch.Size([1, 3, 502, 800])\n",
      "torch.Size([1, 3, 512, 800])\n"
     ]
    }
   ],
   "source": [
    "# Load mutiple images in - https://medium.com/@yvanscher/pytorch-tip-yielding-image-sizes-6a776eb4115b\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "data = ImageFolder('/home/sim/notebooks/relaynet_pytorch/datasets/OCTlabel041818', transform)\n",
    "data_loader = torch.utils.data.DataLoader(dataset=data,\n",
    "                             batch_size=1,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1)\n",
    "\n",
    "\n",
    "for batch in data_loader:\n",
    "    print(batch[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-008dee7be71e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Copy so it doesn't alter init dimensions of test_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# new_test.shape = (1,1,512,600) # Change dimensions of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# new_test = new_test[:400,:400]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# new_test.shape = (1,1,400,400)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "new_test = np.copy(test_data) # Copy so it doesn't alter init dimensions of test_data\n",
    "\n",
    "# new_test.shape = (1,1,512,600) # Change dimensions of data\n",
    "# new_test = new_test[:400,:400]\n",
    "# new_test.shape = (1,1,400,400)\n",
    "new_test = np.transpose(new_test, (0, 1, 3, 2)) # Transposing - changes rotation of image\n",
    "\n",
    "# Getting ReLayNet max_val, idx classification values\n",
    "max_val_trans, idx_trans = relay_out(new_test)\n",
    "\n",
    "# Test image\n",
    "show_image(idx_trans)\n",
    "\n",
    "# Main Image\n",
    "img_test = np.transpose(test_data, (1,0)) # transpose as changes direction of image \n",
    "show_main_image(img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 763876 into shape (1,1,512,600)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-4f0ea4097ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Copy so it doesn't alter init dimensions of test_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Change dimensions of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Getting ReLayNet max_val, idx classification values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmax_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelay_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 763876 into shape (1,1,512,600)"
     ]
    }
   ],
   "source": [
    "new_test = np.copy(test_data) # Copy so it doesn't alter init dimensions of test_data\n",
    "new_test.shape = (1,1,512,600) # Change dimensions of data\n",
    "\n",
    "# Getting ReLayNet max_val, idx classification values\n",
    "max_val, idx = relay_out(new_test)\n",
    "\n",
    "# Test image\n",
    "show_image(idx)\n",
    "\n",
    "# Main Image\n",
    "img_test = test_data\n",
    "show_main_image(img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export Model\n",
    "##### Date:  13/07/2018 \n",
    "Can't export model as MaxPool2d with index output is not supported in ONNX: https://discuss.pytorch.org/t/problems-converting-pytorch-model-into-onnx/12192/3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removed code from batchnorm.py for all references to track_running_stats - see comments in the code\n",
    "\n",
    "# relaynet_model.eval()\n",
    "# dummy_input = Variable(torch.cuda.FloatTensor(1,1,600,64)) # has to be (1,1,...) because the first is no. therefore can't plot 2d image on same plot.\n",
    "# torch.onnx.export(relaynet_model, dummy_input, \"model.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Matlab Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import scipy.io as sio"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
