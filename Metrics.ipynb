{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from PIL import ImageFont\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw\n",
    "import cv2\n",
    "from create_labels import *\n",
    "from stats_helper import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the directories\n",
    "import os\n",
    "\n",
    "wanted_folder = 'alldata/'\n",
    "# wanted_folder = 'pruned/'\n",
    "# wanted_folder = 'Atrium/'\n",
    "# wanted_folder = 'Ventricle/'\n",
    "\n",
    "cwd = os.getcwd()\n",
    "check_directory = cwd\n",
    "if check_directory == '/home/sim/notebooks/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets/OCTData/'+wanted_folder\n",
    "elif check_directory == '/Users/sim/Desktop/Imperial/Project/PreTrained/relaynet_pytorch':\n",
    "    cwd = cwd + '/datasets-24-aug/OCTData/'+wanted_folder\n",
    "\n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "used_image = 1\n",
    "if used_image == 1:\n",
    "    image_file = 'whole_raw_image/con_H1_N01848_LV_1_194.tif'\n",
    "    gnd_truth_image = 'manual_label/label_H1_N01848_LV_1_194.JPG'\n",
    "    gnd_truth_label = 'png_labels_method/label_H1_N01848_LV_1_194_labels.png'\n",
    "    result = 'labels_corrected/label_H1_N01848_LV_1_194_labelscorrected.png'\n",
    "elif used_image == 2:\n",
    "    image_file = 'whole_raw_image/con_H1_N01848_LA_1_272.tif'\n",
    "    gnd_truth_image = 'manual_label/label_H1_N01848_LA_1_272.JPG'\n",
    "    gnd_truth_label = 'png_labels_method/label_H1_N01848_LA_1_272_labels.png'\n",
    "    result = 'labels_corrected/label_H1_N01848_LA_1_272_labelscorrected.png'\n",
    "elif used_image == 3:\n",
    "    image_file = 'whole_raw_image/con_H1_N01848_LA_1_272.tif'\n",
    "    gnd_truth_image = 'manual_label/label_H1_N01848_LA_1_272.JPG'\n",
    "    gnd_truth_label = 'png_labels_method/label_H1_N01848_LA_1_272_labels.png'\n",
    "    result = 'labels_corrected/label_H1_N01848_LV_1_194_labelscorrected.png'\n",
    "# Seeing whether image_file exists\n",
    "raw_image_path = cwd + image_file\n",
    "image = plt.imread(raw_image_path)\n",
    "test_data = image    \n",
    "\n",
    "# Seeing whether labelled_image exists\n",
    "label_image_path = cwd + gnd_truth_image\n",
    "gnd_truth = plt.imread(label_image_path)\n",
    "   \n",
    "# Seeing whether result_image exists\n",
    "gnd_truth_path = cwd + gnd_truth_label\n",
    "gnd = plt.imread(gnd_truth_path)\n",
    "gnd = ((gnd*7)/np.max(values)).astype(int)\n",
    "\n",
    "# Seeing whether result_image exists\n",
    "res_image_path = cwd + result\n",
    "# print(gnd_truth_path)\n",
    "# print(res_image_path)\n",
    "res = plt.imread(res_image_path)\n",
    "res = ((res*7)/np.max(values)).astype(int)\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.imshow(test_data,cmap = \"gray\")\n",
    "plt.title('Raw OCT Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(142)\n",
    "plt.imshow(gnd_truth, cmap = \"gray\")\n",
    "plt.title('Manually Labelled Image'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(143),plt.imshow(gnd,cmap = \"gray\")\n",
    "plt.title('Manually Label ID'), plt.xticks([]), plt.yticks([])\n",
    "plt.subplot(144),plt.imshow(res,cmap = \"gray\")\n",
    "plt.title('Bad Image Label'), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colour = label_img_to_rgb(gnd)\n",
    "# print(gnd[300])\n",
    "# plt.imshow(colour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "File to generate metrics from each of the returned images.\n",
    "\n",
    "* FP & FN\n",
    "* Average Thickness of Layers - similar to MAD-LT below\n",
    "* MAD-LT - error in estimated thickness map (ReLayNet Metric)\n",
    "* DS - Dice Overlap Score (ReLayNet Metric)\n",
    "* Distance (Euclidean) between Ground Truth and Where my Segments are - this is CE - Estimated contour error for each layer (ReLayNet Metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TP, TN, FP, FN, Class Accuracy, Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = list_of_labels(gnd,8)\n",
    "pred_labels = list_of_labels(res,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(20,20))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow((true_labels[:,:,i]), alpha=0.2)\n",
    "    ax.set_title(\"label \" + str(i))\n",
    "    \n",
    "fig, axes = plt.subplots(nrows=1, ncols=8, figsize=(20,20))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow((pred_labels[:,:,i]), alpha=0.2)\n",
    "    ax.set_title(\"label \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_stats(true_labels, pred_labels):\n",
    "    num_classes = 8\n",
    "    class_vals = []\n",
    "    thresh = 0.0001\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        # NOTE: FOR MY CLASSES WITH MANY IMAGES - JUST ADD EXTRA DIMENSION TO pred_labels[x,:,:,i]\n",
    "        # NOTE: if Precision etc are NaN, means there's no information about those classes in this image therefore remove them from analysis of that label\n",
    "\n",
    "        # True Positive (TP): we predict a label of 1 (positive), and the true label is 1.\n",
    "        TP = np.sum(np.logical_and(pred_labels[:,:,i] == 1, true_labels[:,:,i] == 1))\n",
    "\n",
    "        # True Negative (TN): we predict a label of 0 (negative), and the true label is 0.\n",
    "        TN = np.sum(np.logical_and(pred_labels[:,:,i] == 0, true_labels[:,:,i] == 0))\n",
    "\n",
    "        # False Positive (FP): we predict a label of 1 (positive), but the true label is 0.\n",
    "        FP = np.sum(np.logical_and(pred_labels[:,:,i] == 1, true_labels[:,:,i] == 0))\n",
    "\n",
    "        # False Negative (FN): we predict a label of 0 (negative), but the true label is 1.\n",
    "        FN = np.sum(np.logical_and(pred_labels[:,:,i] == 0, true_labels[:,:,i] == 1))\n",
    "\n",
    "        # Accuracy - no of correct predictions / all predictions\n",
    "        Acc = round((TP + TN)/(TP + TN + FP + FN), 3)\n",
    "\n",
    "        # Precision - number of True Positives divided by the number of True Positives and False Positives,\n",
    "        # number of positive predictions divided by the total number of positive class values predicted\n",
    "        Precision = round(TP / (TP + FP),3)\n",
    "\n",
    "        # Recall - number of True Positives divided by the number of True Positives and the number of False Negatives. \n",
    "        # Number of positive predictions divided by the number of positive class values in the test data.\n",
    "        Recall = round(TP / (TP + FN),3)\n",
    "\n",
    "        # F1 Score - 2*((precision*recall)/(precision+recall)). \n",
    "        # F1 score conveys the balance between the precision and the recall.\n",
    "        F1 = round(2 * (Precision * Recall) / (Precision + Recall + thresh), 3) # Dice is Same as F1 Score\n",
    "        # Computing Dice Score\n",
    "        Dice = round(2 * TP / (2*TP+FP+FN),3) \n",
    "\n",
    "        # Dice should return 0 if Precision is low because it means that it classified a value but got it wrong\n",
    "        if Dice != F1:\n",
    "            print(Dice, F1)\n",
    "            print('Dice and F1 not equal')\n",
    "        else:\n",
    "            F1 = Dice\n",
    "\n",
    "#         print('Label:',i)\n",
    "#         print('TP: {}, FP: {}, TN: {}, FN: {}, Class Accuracy: {}, Precision: {}, Recall: {}, Dice: {}'.format(TP,FP,TN,FN,Acc, Precision, Recall, Dice))\n",
    "#         print()\n",
    "        class_vals.append((TP,FP,TN,FN,Acc,Precision,Recall,Dice))\n",
    "    return class_vals\n",
    "find_stats(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Thickness of Layers and Error in Thickness\n",
    "Limitation of this is that if image has classified layer seperately from each other, will still give 1 average thickness score for that column but it's actually 2 different parts of a layer.\n",
    "\n",
    "Assumption is layer will be mostly connecting\n",
    "\n",
    "Return arrays, that we sum down axis to find the avg thickness etc for all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.measure import compare_ssim as ssim\n",
    "\n",
    "\n",
    "\n",
    "def thickness_metrics(true_labels, pred_labels): \n",
    "    '''\n",
    "    Takes True and Predicted Labels that are one hot encoded\n",
    "    returns list of avg true thickness, avg pred thickness, \n",
    "    '''\n",
    "    avg_true_thickness_list = []\n",
    "    avg_pred_thickness_list = []\n",
    "    mean_abs_error_list = [] \n",
    "    mean_squared_error_list = [] \n",
    "    ssim_list = []\n",
    "    \n",
    "    N = 512\n",
    "    error_of_thickness = []\n",
    "    for i in range(8):\n",
    "        # NOTE: IF AVERAGE_TIHCKNESS IS NAN, MEANS NOT IN THIS IMAGE\n",
    "        true_thickness = []\n",
    "        pred_thickness = []\n",
    "        class_error = []\n",
    "        # For each col, find thickness, compare to actual thickness and sum errors \n",
    "        for j in range(N):\n",
    "            true_col = true_labels[:,j,i] # finding number of values in col, go down axis i.e. index of axis\n",
    "            pred_col = pred_labels[:,j,i] \n",
    "\n",
    "            true_width = numpy.count_nonzero(true_col) # count number of 1s\n",
    "            pred_width = numpy.count_nonzero(pred_col)\n",
    "\n",
    "            # Finding thickness by looking at pred_width - don't need truth because will find error\n",
    "            if true_width != 0:\n",
    "                true_thickness.append(pred_width)\n",
    "            if pred_width != 0:\n",
    "                pred_thickness.append(pred_width)\n",
    "\n",
    "            # If true width is not 0 or pred_width is not 0, append them otherwise there's no label for this image\n",
    "            if true_width != 0 or pred_width != 0:\n",
    "                abs_error = abs(true_width - pred_width) # error is true - pred\n",
    "                class_error.append(abs_error)\n",
    "\n",
    "        avg_true_thickness = np.average(true_thickness)\n",
    "        avg_pred_thickness = np.average(pred_thickness)\n",
    "        mean_abs_error = np.average(class_error)\n",
    "        mean_squared_error = np.average(np.power(class_error,2))\n",
    "        s = ssim(true_labels[:,:,i], pred_labels[:,:,i])\n",
    "        \n",
    "        #print('Label: {} \\nAverage True Thickness: {}' \\\n",
    "        #      '\\nAverage Predicted Thickness: {}' \\\n",
    "        #      '\\nMean Absolute Error of Thickness: {}'\\\n",
    "        #      '\\nMean Squared Error of Thickness: {}'\\\n",
    "        #      '\\nSSIM: {}\\n'\\\n",
    "        #      .format(i,avg_true_thickness,avg_pred_thickness,mean_abs_error, mean_squared_error, s))\n",
    "        \n",
    "        avg_true_thickness_list.append(avg_true_thickness)\n",
    "        avg_pred_thickness_list.append(avg_pred_thickness)\n",
    "        mean_abs_error_list.append(mean_abs_error) \n",
    "        mean_squared_error_list.append(mean_squared_error) \n",
    "        ssim_list.append(s)\n",
    "\n",
    "    return avg_true_thickness_list,avg_pred_thickness_list,mean_abs_error_list, mean_squared_error_list, ssim_list\n",
    "\n",
    "avg_true_thickness_list = []\n",
    "avg_pred_thickness_list = []\n",
    "mean_abs_error_list = [] \n",
    "mean_squared_error_list = [] \n",
    "ssim_list = []\n",
    "\n",
    "for i in range(2):\n",
    "    avg_true_thickness,avg_pred_thickness,mean_abs_error, mean_squared_error, s = thickness_metrics(true_labels, pred_labels)\n",
    "    avg_true_thickness_list.append(avg_true_thickness)\n",
    "    avg_pred_thickness_list.append(avg_pred_thickness)\n",
    "    mean_abs_error_list.append(mean_abs_error) \n",
    "    mean_squared_error_list.append(mean_squared_error) \n",
    "    ssim_list.append(s)\n",
    "# print(avg_true_thickness_list,avg_pred_thickness_list,mean_abs_error_list, mean_squared_error_list, ssim_list)\n",
    "print(len(avg_true_thickness_list))\n",
    "print(np.average(avg_true_thickness_list, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dice Score \n",
    "\n",
    "Source: https://stats.stackexchange.com/questions/195006/is-the-dice-coefficient-the-same-as-accuracy\n",
    "\n",
    "* Dice Score: relaynet_pytorch - solver.py in train function.\n",
    "* Dice Score: networks -> net_api -> losses.py in class DiceCoeff as well as DiceLoss\n",
    "* TF Dice Score: Line 42 in ReLayNey_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dice(true_list, pred_list):\n",
    "    k = 1\n",
    "    scores = []\n",
    "    for i in range(8):\n",
    "        gt = true_list[:,:,i]\n",
    "        seg = pred_list[:,:,i]\n",
    "        dice = np.sum(seg[gt==k]==k)*2.0 / (np.sum(seg[seg==k]==k) + np.sum(gt[gt==k]==k))\n",
    "        dice = round(dice,2)\n",
    "#         print('Dice similarity score is {}'.format(dice))\n",
    "#         if dice > -0.00001:\n",
    "        scores.append(dice)\n",
    "#     print('Average Dice Score:', np.average(scores))\n",
    "    return scores\n",
    "score = compute_dice(true_labels, pred_labels)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
